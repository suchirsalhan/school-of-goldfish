# -*- coding: utf-8 -*-
"""Blimp eng

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PnhXvGmXNA1hcaWH5f6G2bcl2ljKLQad
"""

!pip install transformers torch tqdm huggingface_hub pandas

!pip install pandas

!git clone https://github.com/alexwarstadt/blimp.git

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

model_name = "catherinearnett/B-GPT_es_en_sequential"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
model.eval()

import torch

@torch.no_grad()
def sentence_logprob(sentence):
    # Tokenize input sentence
    enc = tokenizer(sentence, return_tensors="pt")
    input_ids = enc.input_ids.to(device)

    # Forward pass
    outputs = model(input_ids)
    logits = outputs.logits

    # Shift for causal LM
    shift_logits = logits[:, :-1, :]
    shift_labels = input_ids[:, 1:]

    log_probs = torch.log_softmax(shift_logits, dim=-1)

    # Select log-probabilities of the correct next tokens
    token_log_probs = log_probs.gather(
        2, shift_labels.unsqueeze(-1)
    ).squeeze(-1)

    # Sum over tokens
    return token_log_probs.sum().item()

import json

def load_blimp_file(path):
    with open(path, "r") as f:
        return [json.loads(line) for line in f]

pairs = load_blimp_file("blimp/data/adjunct_island.jsonl")
print(len(pairs))  # should print 1000

from tqdm import tqdm

def evaluate_paradigm(pairs):
    correct = 0

    for ex in tqdm(pairs):
        good_lp = sentence_logprob(ex["sentence_good"])
        bad_lp  = sentence_logprob(ex["sentence_bad"])

        if good_lp > bad_lp:
            correct += 1

    return correct / len(pairs)

acc = evaluate_paradigm(pairs)
print("Adjunct Island Accuracy:", acc)

import glob
import os

results = {}

for path in sorted(glob.glob("blimp/data/*.jsonl")):
    name = os.path.basename(path).replace(".jsonl", "")
    print(f"Evaluating {name}...")
    pairs = load_blimp_file(path)
    acc = evaluate_paradigm(pairs)
    results[name] = acc

for paradigm, acc in sorted(results.items()):
    print(f"{paradigm:45s} {acc:.4f}")

overall_accuracy = sum(results.values()) / len(results)
print("Overall BLiMP Accuracy:", overall_accuracy)